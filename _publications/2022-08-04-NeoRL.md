---
title: "NeoRL: A near real-world benchmark for offline reinforcement learning"
collection: publications
permalink: /publication/2022-08-NeoRL
excerpt: 'NeoRL presents conservative datasets for offline RL, highlights the complete pipeline for deploying offline RL in real-world applications, and also benchmarks recent offline RL algorithms on NeoRL under the complete pipeline.'
date: 2022-08-04
venue: 'NeurIPS 2022, Datasets and Benchmarks Track'
paperurl: 'https://openreview.net/pdf?id=jNdLszxdtra'
citation: 'Qin, R. J., Zhang, X., Gao, S., Chen, X. H., Li, Z., Zhang, W., & Yu, Y. (2022). NeoRL: A near real-world benchmark for offline reinforcement learning. Advances in Neural Information Processing Systems, 35, 24753-24765.'
---
Offline reinforcement learning (RL) aims at learning effective policies from historical data without extra environment interactions. During our experience of applying offline RL, we noticed that previous offline RL benchmarks commonly involve significant reality gaps, which we have identified include rich and overly exploratory datasets, degraded baseline, and missing policy validation. In many real-world situations, to ensure system safety, running an overly exploratory policy to collect various data is prohibited, thus only a narrow data distribution is available. The resulting policy is regarded as effective if it is better than the working behavior policy; the policy model can be deployed only if it has been well validated, rather than accomplished the training. In this paper, we present a Near real-world offline RL benchmark, named NeoRL, to reflect these properties. NeoRL datasets are collected with a more conservative strategy. Moreover, NeoRL contains the offline training and offline validation pipeline before the online test, corresponding to real-world situations. We then evaluate recent state-of-the-art offline RL algorithms in NeoRL. The empirical results demonstrate that some offline RL algorithms are less competitive to the behavior cloning and the deterministic behavior policy, implying that they could be less effective in real-world tasks than in the previous benchmarks. We also disclose that current offline policy evaluation methods could hardly select the best policy. We hope this work will shed some light on future research and deploying RL in real-world systems.

[Download paper here](https://openreview.net/pdf?id=jNdLszxdtra)

Recommended citation: Qin, R. J., Zhang, X., Gao, S., Chen, X. H., Li, Z., Zhang, W., & Yu, Y. (2022). NeoRL: A near real-world benchmark for offline reinforcement learning. Advances in Neural Information Processing Systems, 35, 24753-24765.